{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10)) # default figure size\n",
    "    for i, num in enumerate(slice_nums): # iterate over ordinated slices\n",
    "        plt.subplot(1, len(slice_nums), i + 1) # set up i+1-th subplot in a grid with height 1 and width len\n",
    "        plt.imshow(data[num], cmap=cmap) # plot the data of each slice as cmap\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {} # initiate the data list\n",
    "    train_and_val = ['train', 'val'] # set the keys in data list\n",
    "    data_path = [train_data_path, val_data_path] # import the data path for both training and validation\n",
    "      \n",
    "    for i in range(len(data_path)): # iterate over both paths\n",
    "\n",
    "        data_list[train_and_val[i]] = [] # reset the data list \n",
    "        \n",
    "        which_data_path = data_path[i] # set the data path to current dataset\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)): # iterate over all listed files in the data path\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path, fname) # set subject data path with its directory\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue # iterate until all files are registered on path\n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data: # open the subject at the desinated path\n",
    "                num_slice = data['kspace'].shape[0] # set # of slices as the 1st-D of the data's kspace data\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "            # data_list is concatenated by a list each consisted with a file's name, path and slices excluding \n",
    "            # the first 5 slices that are  mostly considered as noise \n",
    "            \n",
    "    return data_list # {'train': [('file1000000.h5','/home/kevinxu/Documents/NC2019MRI/train/file1000000.h5',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list # data list imported to be processed\n",
    "        self.acceleration = acceleration # defined AF\n",
    "        self.center_fraction = center_fraction # defined CF\n",
    "        self.use_seed = use_seed # defined random or stable mask\n",
    "\n",
    "    def __len__(self): # create an attribute of the length of dataset\n",
    "        return len(self.data_list) \n",
    "\n",
    "    def __getitem__(self, idx): # return an item when called with an index\n",
    "        subject_id = self.data_list[idx] # set subject id as the called index\n",
    "        \n",
    "        # return ground truth, under-sampled real image, masked kspace data, masks and norm\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_path, slice = subject_id # read file name, path, slice\n",
    "    \n",
    "    with h5py.File(rawdata_path, 'r') as data:\n",
    "        rawdata = data['kspace'][slice] # return a random slice of the kspace data\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0) # transform the slice to tensor and add a dimension\n",
    "    S, Ny, Nx, ps = slice_kspace.shape # assign parameters according to the kspace shape; S = batch_size\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape) \n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc]) # see subsample.py\n",
    "    seed = None if not use_seed else tuple(map(ord, fname)) # generate a tuple of fname unicode for seeding\n",
    "    mask = mask_func(shape, seed) # generate the mask as the shape [1,1,num_col,1]\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace) # apply mask to kspace slice\n",
    "    masks = mask.repeat(S, Ny, 1, ps) # save the mask and expand its shape to fit sample [S, Ny, num_col, ps]\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace) # ground truth and under-sampled image\n",
    "\n",
    "    ## perform data normalization which is important for network to learn useful features\n",
    "    ## during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max() # set norm as the max value of sqrt((img_und)**2)\n",
    "    if norm < 1e-6: norm = 1e-6 # avoid value error of being divided by zero\n",
    "    \n",
    "    # normalized data: ground truth, under-sampled real image, masked kspace data, respectively\n",
    "    img_gt, img_und, masked_kspace = img_gt/norm, img_und/norm, masked_kspace/norm \n",
    "    \n",
    "    # return ground truth, under-sampled real image, masked kspace data, masks and norm\n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), masked_kspace.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # main() function of the file\n",
    "    \n",
    "    data_path_train = '/home/kevinxu/Documents/NC2019MRI/train'\n",
    "    data_path_val = '/home/kevinxu/Documents/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "\n",
    "    acc = 8 # AF\n",
    "    cen_fract = 0.04 # central fraction that are set to be included\n",
    "    seed = False # random masks for each slice \n",
    "    num_batch = 1 # batch size of each loading process\n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using cpu.\n",
    "    \n",
    "    # create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=num_batch, num_workers=num_workers) \n",
    "    for iteration, sample in enumerate(train_loader): # iterate over every element in the train_loader\n",
    "        img_gt, img_und, masked_kspace, masks, norm = sample\n",
    "        \n",
    "        # stack different slices into a volume for visualisation\n",
    "        A = masks[...,0].squeeze()    # mask applied\n",
    "        B = torch.log(T.complex_abs(masked_kspace) + 1e-9).squeeze()    # masked kspace image\n",
    "        C = T.center_crop(T.complex_abs(img_und), [320,320])    # image generated from under-sampled data\n",
    "        D = T.complex_abs(img_gt).squeeze()    # image generated from ground truth data\n",
    "        # all_imgs = torch.stack([A,B,C,D], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 368])\n",
      "torch.Size([640, 368])\n",
      "torch.Size([1, 320, 320])\n",
      "torch.Size([640, 368])\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)\n",
    "print(B.shape)\n",
    "print(C.shape)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module): # the cnn block used in u-net architecture\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob # parameter defining the probability of dropout in each layer\n",
    "\n",
    "        self.layers = nn.Sequential( # create layers\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1), # convolution layer\n",
    "            nn.InstanceNorm2d(out_chans), # instance normalization\n",
    "            nn.ReLU(), # relu activation\n",
    "            nn.Dropout2d(drop_prob), # dropout regularisation\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1), # the second layer\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, ' \\\n",
    "            f'drop_prob={self.drop_prob})'\n",
    "\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "\n",
    "    This is based on:\n",
    "        Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks\n",
    "        for biomedical image segmentation. In International Conference on Medical image\n",
    "        computing and computer-assisted intervention, pages 234â€“241. Springer, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # down-sample cnn layers\n",
    "        self.down_sample_layers = nn.ModuleList([ConvBlock(in_chans, chans, drop_prob)])\n",
    "        ch = chans # number of output channels of the first convolution layer\n",
    "        for i in range(num_pool_layers - 1): # iterate over number of total pooling layers\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob)] # update down-sample layers x2\n",
    "            ch *= 2 # double the output channels of the cnn layers\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob) # update conv layer with ch\n",
    "\n",
    "        # up-sample cnn layers\n",
    "        self.up_sample_layers = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob)] # update up-sample layers //4\n",
    "            ch //= 2 # update ch with ch floordiv by 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob)] # update up-sample layers //2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1), \n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1), # conv filter with output shape\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output) # fit layers\n",
    "            stack.append(output) # append new output in every iteration to stack []\n",
    "            output = F.max_pool2d(output, kernel_size=2) # apply max-pool\n",
    "\n",
    "        output = self.conv(output) # apply conv block\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False) # upsample\n",
    "            output = torch.cat([output, stack.pop()], dim=1) # concatenate output with the last stack on dim=1\n",
    "            output = layer(output) # fit layers\n",
    "        return self.conv2(output) # apply up-sampling layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UnetModel(in_chans=1, out_chans=1, chans=32, num_pool_layers=4, drop_prob=0.5).to(device)\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_count = []\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_gt, img_und, masked_kspace, masks, norm = sample\n",
    "        \n",
    "        img_und = T.complex_abs(img_und)\n",
    "        img_gt = T.complex_abs(img_gt)\n",
    "        \n",
    "        img_und = img_und.unsqueeze(0)\n",
    "        img_gt = img_gt.unsqueeze(0)\n",
    "       \n",
    "        img_und = T.center_crop(img_und, [320, 320])\n",
    "        img_gt = T.center_crop(img_gt, [320, 320])\n",
    "        \n",
    "        X = img_und\n",
    "        Y = img_gt\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        output = model(X)\n",
    "        loss = F.l1_loss(output, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_count.append(loss)\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1451922059059143\n",
      "0.13399459421634674\n",
      "0.226047083735466\n",
      "0.18528619408607483\n",
      "0.22873082756996155\n",
      "0.162269726395607\n",
      "0.1423427164554596\n",
      "0.1563226729631424\n",
      "0.10322114825248718\n",
      "0.14555524289608002\n",
      "0.1403808295726776\n",
      "0.16193753480911255\n",
      "0.2081247717142105\n",
      "0.13759863376617432\n",
      "0.25448372960090637\n",
      "0.23718616366386414\n",
      "0.10618693381547928\n",
      "0.09569073468446732\n",
      "0.17723172903060913\n",
      "0.13784416019916534\n",
      "0.18521665036678314\n",
      "0.22095516324043274\n",
      "0.19658885896205902\n",
      "0.14369629323482513\n",
      "0.23650585114955902\n",
      "0.14083686470985413\n",
      "0.15121260285377502\n",
      "0.18356284499168396\n",
      "0.20753280818462372\n",
      "0.22143849730491638\n",
      "0.21853795647621155\n",
      "0.15041939914226532\n",
      "0.20778389275074005\n",
      "0.1366192102432251\n",
      "0.17474672198295593\n",
      "0.14438188076019287\n",
      "0.1215292364358902\n",
      "0.13965822756290436\n",
      "0.2827502191066742\n",
      "0.24032673239707947\n",
      "0.16718004643917084\n",
      "0.1054491251707077\n",
      "0.1708657145500183\n",
      "0.15838216245174408\n",
      "0.10379153490066528\n",
      "0.1550397276878357\n",
      "0.1401236653327942\n",
      "0.2539450526237488\n",
      "0.20414745807647705\n",
      "0.22857467830181122\n",
      "0.23349153995513916\n",
      "0.19641853868961334\n",
      "0.15734602510929108\n",
      "0.13879838585853577\n",
      "0.2619732618331909\n",
      "0.2204650491476059\n",
      "0.14891469478607178\n",
      "0.1720970869064331\n",
      "0.21404778957366943\n",
      "0.2463156133890152\n",
      "0.14618530869483948\n",
      "0.14179037511348724\n",
      "0.22460795938968658\n",
      "0.1644902378320694\n",
      "0.13469402492046356\n",
      "0.13240212202072144\n",
      "0.2234923243522644\n",
      "0.1343107670545578\n",
      "0.15773703157901764\n",
      "0.26257285475730896\n",
      "0.10005777329206467\n",
      "0.17056088149547577\n",
      "0.1590787172317505\n",
      "0.166653111577034\n",
      "0.18737222254276276\n",
      "0.144847109913826\n",
      "0.11834172904491425\n",
      "0.13386978209018707\n",
      "0.2588246464729309\n",
      "0.16705259680747986\n",
      "0.1402234435081482\n",
      "0.18789657950401306\n",
      "0.1675671637058258\n",
      "0.17964178323745728\n",
      "0.17830242216587067\n",
      "0.25963112711906433\n",
      "0.14142192900180817\n",
      "0.1389131098985672\n",
      "0.11757652461528778\n",
      "0.16986694931983948\n",
      "0.15502648055553436\n",
      "0.17510704696178436\n",
      "0.24688620865345\n",
      "0.14005136489868164\n",
      "0.11961719393730164\n",
      "0.2868083417415619\n",
      "0.19213184714317322\n",
      "0.20438073575496674\n",
      "0.16859525442123413\n",
      "0.21127921342849731\n",
      "0.2781837582588196\n",
      "0.21660782396793365\n",
      "0.14463013410568237\n",
      "0.1183810606598854\n",
      "0.20736263692378998\n",
      "0.12212623655796051\n",
      "0.11412589997053146\n",
      "0.19285732507705688\n",
      "0.11079219728708267\n",
      "0.17439401149749756\n",
      "0.17608745396137238\n",
      "0.1157665029168129\n",
      "0.17307204008102417\n",
      "0.20740367472171783\n",
      "0.28625279664993286\n",
      "0.1881413608789444\n",
      "0.16473805904388428\n",
      "0.21937553584575653\n",
      "0.21295028924942017\n",
      "0.12459976971149445\n",
      "0.1092788577079773\n",
      "0.20448318123817444\n",
      "0.11978387832641602\n",
      "0.12202344834804535\n",
      "0.13056541979312897\n",
      "0.19178222119808197\n",
      "0.18008989095687866\n",
      "0.14474515616893768\n",
      "0.21361759305000305\n",
      "0.14506736397743225\n",
      "0.13200904428958893\n",
      "0.24082785844802856\n",
      "0.22031769156455994\n",
      "0.15743182599544525\n",
      "0.1672293096780777\n",
      "0.16572844982147217\n",
      "0.1919020116329193\n",
      "0.13629020750522614\n",
      "0.15436138212680817\n",
      "0.14279599487781525\n",
      "0.13092955946922302\n",
      "0.23941051959991455\n",
      "0.11789488047361374\n",
      "0.22700904309749603\n",
      "0.23739591240882874\n",
      "0.24718433618545532\n",
      "0.14652901887893677\n",
      "0.19136552512645721\n",
      "0.18939901888370514\n",
      "0.1967969387769699\n",
      "0.1541464626789093\n",
      "0.24065342545509338\n",
      "0.20318534970283508\n",
      "0.10422462224960327\n",
      "0.14994725584983826\n",
      "0.11353370547294617\n",
      "0.1849447637796402\n",
      "0.20682348310947418\n",
      "0.19594649970531464\n",
      "0.15250138938426971\n",
      "0.22037719190120697\n",
      "0.1719502955675125\n",
      "0.17122316360473633\n",
      "0.2556169629096985\n",
      "0.19197070598602295\n",
      "0.15888705849647522\n",
      "0.1778838038444519\n",
      "0.1469494104385376\n",
      "0.18460233509540558\n",
      "0.11424963921308517\n",
      "0.1978292018175125\n",
      "0.13379007577896118\n",
      "0.10694780945777893\n",
      "0.16543835401535034\n",
      "0.1570657342672348\n",
      "0.1288759857416153\n",
      "0.20475071668624878\n",
      "0.22488246858119965\n",
      "0.189973846077919\n",
      "0.2103952020406723\n",
      "0.10719718784093857\n",
      "0.22580833733081818\n",
      "0.15408025681972504\n",
      "0.1395759880542755\n",
      "0.18876315653324127\n",
      "0.12312067300081253\n",
      "0.12125635147094727\n",
      "0.12921567261219025\n",
      "0.21090508997440338\n",
      "0.20885005593299866\n",
      "0.19043682515621185\n",
      "0.22432294487953186\n",
      "0.17490684986114502\n",
      "0.17994725704193115\n",
      "0.11531771719455719\n",
      "0.21428653597831726\n",
      "0.1818019300699234\n",
      "0.15076448023319244\n",
      "0.2425379604101181\n",
      "0.2369905263185501\n",
      "0.15812568366527557\n",
      "0.2441703826189041\n",
      "0.26962700486183167\n",
      "0.14063671231269836\n",
      "0.09620413929224014\n",
      "0.13549087941646576\n",
      "0.12669411301612854\n",
      "0.2331262230873108\n",
      "0.28740987181663513\n",
      "0.2740108072757721\n",
      "0.15367113053798676\n",
      "0.13435183465480804\n",
      "0.1391758769750595\n",
      "0.14585989713668823\n",
      "0.1308814287185669\n",
      "0.20306077599525452\n",
      "0.196482315659523\n",
      "0.21994967758655548\n",
      "0.15319178998470306\n",
      "0.16817516088485718\n",
      "0.14465436339378357\n",
      "0.11081359535455704\n",
      "0.232122540473938\n",
      "0.11239883303642273\n",
      "0.252678781747818\n",
      "0.12828226387500763\n",
      "0.11463505774736404\n",
      "0.2095397561788559\n",
      "0.19759373366832733\n",
      "0.15743665397167206\n",
      "0.3125518262386322\n",
      "0.21680867671966553\n",
      "0.15855497121810913\n",
      "0.12279746681451797\n",
      "0.1776495724916458\n",
      "0.13349714875221252\n",
      "0.1710222214460373\n",
      "0.0947442427277565\n",
      "0.13961035013198853\n",
      "0.28019630908966064\n",
      "0.2593761682510376\n",
      "0.11292831599712372\n",
      "0.1668771505355835\n",
      "0.1398303359746933\n",
      "0.17098963260650635\n",
      "0.1747428923845291\n",
      "0.11019548773765564\n",
      "0.13413149118423462\n",
      "0.11720919609069824\n",
      "0.1299775093793869\n",
      "0.14940471947193146\n",
      "0.17978553473949432\n",
      "0.12004703283309937\n",
      "0.19143886864185333\n",
      "0.12134136259555817\n",
      "0.14074836671352386\n",
      "0.24208411574363708\n",
      "0.14966721832752228\n",
      "0.16215339303016663\n",
      "0.13300573825836182\n",
      "0.2456749677658081\n",
      "0.1058899313211441\n",
      "0.17550545930862427\n",
      "0.2062578946352005\n",
      "0.1826906055212021\n",
      "0.17683708667755127\n",
      "0.24317854642868042\n",
      "0.11988139897584915\n",
      "0.1386968046426773\n",
      "0.13742971420288086\n",
      "0.16580447554588318\n",
      "0.17374032735824585\n",
      "0.16001877188682556\n",
      "0.21339485049247742\n",
      "0.12527962028980255\n",
      "0.18051820993423462\n",
      "0.10898198932409286\n",
      "0.18491555750370026\n",
      "0.15766321122646332\n",
      "0.22511427104473114\n",
      "0.2326827198266983\n",
      "0.13375450670719147\n",
      "0.14161598682403564\n",
      "0.18218925595283508\n",
      "0.12970595061779022\n",
      "0.1730061024427414\n",
      "0.1881200224161148\n",
      "0.16793687641620636\n",
      "0.1334225833415985\n",
      "0.15378011763095856\n",
      "0.11070964485406876\n",
      "0.21067191660404205\n",
      "0.20617081224918365\n",
      "0.19439181685447693\n",
      "0.1998516470193863\n",
      "0.23091506958007812\n",
      "0.09072620421648026\n",
      "0.19417518377304077\n",
      "0.24002815783023834\n",
      "0.21824035048484802\n",
      "0.24780642986297607\n",
      "0.17563992738723755\n",
      "0.11024462431669235\n",
      "0.1166113093495369\n",
      "0.1576583981513977\n",
      "0.16832482814788818\n",
      "0.10194385796785355\n",
      "0.16405706107616425\n",
      "0.2659471929073334\n",
      "0.12818992137908936\n",
      "0.16764485836029053\n",
      "0.10667069256305695\n",
      "0.11755753308534622\n",
      "0.18212667107582092\n",
      "0.2283841073513031\n",
      "0.08849658071994781\n",
      "0.21494188904762268\n",
      "0.254597544670105\n",
      "0.2246904969215393\n",
      "0.16177551448345184\n",
      "0.1534947007894516\n",
      "0.23631584644317627\n",
      "0.0814455896615982\n",
      "0.1234462633728981\n",
      "0.17324204742908478\n",
      "0.12501823902130127\n",
      "0.11733125895261765\n",
      "0.16127094626426697\n",
      "0.1316520720720291\n",
      "0.23138660192489624\n",
      "0.10344009101390839\n",
      "0.16137422621250153\n",
      "0.09604813903570175\n",
      "0.17768609523773193\n",
      "0.16753889620304108\n",
      "0.15941743552684784\n",
      "0.15552683174610138\n",
      "0.2095402479171753\n",
      "0.14795413613319397\n",
      "0.2722814679145813\n",
      "0.15042085945606232\n",
      "0.16443166136741638\n",
      "0.17476624250411987\n",
      "0.2117334008216858\n",
      "0.12867213785648346\n",
      "0.11399145424365997\n",
      "0.16531173884868622\n",
      "0.17606712877750397\n",
      "0.09847820550203323\n",
      "0.14184243977069855\n",
      "0.10595947504043579\n",
      "0.13404954969882965\n",
      "0.12887579202651978\n",
      "0.1549469530582428\n",
      "0.13075733184814453\n",
      "0.17342929542064667\n",
      "0.1400812715291977\n",
      "0.2125207483768463\n",
      "0.1646064817905426\n",
      "0.28368428349494934\n",
      "0.09955095499753952\n",
      "0.23451393842697144\n",
      "0.12400541454553604\n",
      "0.1636577546596527\n",
      "0.13267751038074493\n",
      "0.11010459810495377\n",
      "0.1742645651102066\n",
      "0.2276618927717209\n",
      "0.13213396072387695\n",
      "0.11976800858974457\n",
      "0.08212980628013611\n",
      "0.13730773329734802\n",
      "0.173492893576622\n",
      "0.1509142518043518\n",
      "0.12134179472923279\n",
      "0.10280473530292511\n",
      "0.1524214744567871\n",
      "0.08819538354873657\n",
      "0.16554687917232513\n",
      "0.1766366958618164\n",
      "0.14474478363990784\n",
      "0.07536423951387405\n",
      "0.19148288667201996\n",
      "0.1056298092007637\n",
      "0.16801542043685913\n",
      "0.12213938683271408\n",
      "0.11041844636201859\n",
      "0.10692202299833298\n",
      "0.16434143483638763\n",
      "0.12514598667621613\n",
      "0.14730724692344666\n",
      "0.12357965111732483\n",
      "0.10593605041503906\n",
      "0.11537037044763565\n",
      "0.1626332849264145\n",
      "0.12150205671787262\n",
      "0.09468910098075867\n",
      "0.23367004096508026\n",
      "0.2097419798374176\n",
      "0.21460561454296112\n",
      "0.23227186501026154\n",
      "0.2228686511516571\n",
      "0.14551004767417908\n",
      "0.09610781073570251\n",
      "0.1707378774881363\n",
      "0.14363956451416016\n",
      "0.07855598628520966\n",
      "0.10532606393098831\n",
      "0.1885470747947693\n",
      "0.059911955147981644\n",
      "0.1441243439912796\n",
      "0.18605157732963562\n",
      "0.11272163689136505\n",
      "0.11097169667482376\n",
      "0.15318304300308228\n",
      "0.12560562789440155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1524774581193924\n",
      "0.1370285004377365\n",
      "0.08900697529315948\n",
      "0.142831951379776\n",
      "0.12230705469846725\n",
      "0.22308143973350525\n",
      "0.19997093081474304\n",
      "0.23094642162322998\n",
      "0.14084650576114655\n",
      "0.12820014357566833\n",
      "0.13570408523082733\n",
      "0.1327357143163681\n",
      "0.10806754231452942\n",
      "0.10209755599498749\n",
      "0.08114292472600937\n",
      "0.12111146748065948\n",
      "0.09899626672267914\n",
      "0.09874363988637924\n",
      "0.10925010591745377\n",
      "0.0940852165222168\n",
      "0.1796237975358963\n",
      "0.1672094762325287\n",
      "0.17968136072158813\n",
      "0.09341632574796677\n",
      "0.08765564113855362\n",
      "0.15079770982265472\n",
      "0.12796525657176971\n",
      "0.08115202188491821\n",
      "0.1462436318397522\n",
      "0.15172742307186127\n",
      "0.1570253223180771\n",
      "0.12998832762241364\n",
      "0.11610017716884613\n",
      "0.12353706359863281\n",
      "0.1077425479888916\n",
      "0.10800345242023468\n",
      "0.103665791451931\n",
      "0.18247123062610626\n",
      "0.1459828019142151\n",
      "0.11274610459804535\n",
      "0.13646499812602997\n",
      "0.09665622562170029\n",
      "0.12567879259586334\n",
      "0.1730988472700119\n",
      "0.1403058022260666\n",
      "0.11699961870908737\n",
      "0.163601815700531\n",
      "0.25615614652633667\n",
      "0.12799270451068878\n",
      "0.13245010375976562\n",
      "0.11774880439043045\n",
      "0.11807440966367722\n",
      "0.11256060004234314\n",
      "0.09406857192516327\n",
      "0.11728627234697342\n",
      "0.1261328160762787\n",
      "0.11870267242193222\n",
      "0.12282044440507889\n",
      "0.1294693499803543\n",
      "0.113213449716568\n",
      "0.2393019050359726\n",
      "0.11019009351730347\n",
      "0.10080473124980927\n",
      "0.0902838483452797\n",
      "0.13372072577476501\n",
      "0.1659858524799347\n",
      "0.11776769906282425\n",
      "0.07485055178403854\n",
      "0.12305328249931335\n",
      "0.18212780356407166\n",
      "0.1800166368484497\n",
      "0.2388666570186615\n",
      "0.13969038426876068\n",
      "0.13106735050678253\n",
      "0.17557506263256073\n",
      "0.1348995715379715\n",
      "0.09595669060945511\n",
      "0.09143462032079697\n",
      "0.19303110241889954\n",
      "0.15566511452198029\n",
      "0.08928146213293076\n",
      "0.08493649214506149\n",
      "0.11168696731328964\n",
      "0.1539894938468933\n",
      "0.2598613500595093\n",
      "0.14979518949985504\n",
      "0.10364240407943726\n",
      "0.11038453876972198\n",
      "0.1753816157579422\n",
      "0.15521378815174103\n",
      "0.09730291366577148\n",
      "0.12159869819879532\n",
      "0.08785135298967361\n",
      "0.12495021522045135\n",
      "0.10889707505702972\n",
      "0.09831864386796951\n",
      "0.11392760276794434\n",
      "0.18288780748844147\n",
      "0.13696366548538208\n",
      "0.12890580296516418\n",
      "0.1252840906381607\n",
      "0.13442321121692657\n",
      "0.21830818057060242\n",
      "0.11343397945165634\n",
      "0.09927743673324585\n",
      "0.20556312799453735\n",
      "0.12487766891717911\n",
      "0.11447395384311676\n",
      "0.17937375605106354\n",
      "0.12232811003923416\n",
      "0.15920062363147736\n",
      "0.14051112532615662\n",
      "0.09452018141746521\n",
      "0.1721193939447403\n",
      "0.20854058861732483\n",
      "0.1561816781759262\n",
      "0.19933931529521942\n",
      "0.10599413514137268\n",
      "0.10979662835597992\n",
      "0.10270673781633377\n",
      "0.17100635170936584\n",
      "0.12120754271745682\n",
      "0.15628530085086823\n",
      "0.11724314093589783\n",
      "0.11778996139764786\n",
      "0.13544568419456482\n",
      "0.10494432598352432\n",
      "0.08319687098264694\n",
      "0.09437744319438934\n",
      "0.11640982329845428\n",
      "0.13138017058372498\n",
      "0.14607544243335724\n",
      "0.11645099520683289\n",
      "0.1218603104352951\n",
      "0.08875646442174911\n",
      "0.14052852988243103\n",
      "0.2075522243976593\n",
      "0.12285446375608444\n",
      "0.26203811168670654\n",
      "0.12647518515586853\n",
      "0.12714694440364838\n",
      "0.15853889286518097\n",
      "0.11342010647058487\n",
      "0.16633784770965576\n",
      "0.15149740874767303\n",
      "0.12443208694458008\n",
      "0.19393864274024963\n",
      "0.10112079977989197\n",
      "0.1388741284608841\n",
      "0.1675231158733368\n",
      "0.12874096632003784\n",
      "0.11502566188573837\n",
      "0.09777185320854187\n",
      "0.11234086751937866\n",
      "0.21110586822032928\n",
      "0.12525366246700287\n",
      "0.10148181766271591\n",
      "0.12895339727401733\n",
      "0.10104190558195114\n",
      "0.1213943362236023\n",
      "0.11312977969646454\n",
      "0.11679088324308395\n",
      "0.11359178274869919\n",
      "0.13640643656253815\n",
      "0.10242221504449844\n",
      "0.11434908956289291\n",
      "0.09626644104719162\n",
      "0.11298684775829315\n",
      "0.1728915423154831\n",
      "0.12165090441703796\n",
      "0.15432322025299072\n",
      "0.1536502093076706\n",
      "0.14891931414604187\n",
      "0.09843747317790985\n",
      "0.11480510979890823\n",
      "0.13562937080860138\n",
      "0.1964777112007141\n",
      "0.1137113943696022\n",
      "0.20746910572052002\n",
      "0.11769261956214905\n",
      "0.1102742925286293\n",
      "0.08047132194042206\n",
      "0.14992748200893402\n",
      "0.12462203204631805\n",
      "0.11375982314348221\n",
      "0.1584940254688263\n",
      "0.16335223615169525\n",
      "0.14907406270503998\n",
      "0.10214702785015106\n",
      "0.09465761482715607\n",
      "0.10189452022314072\n",
      "0.0913308784365654\n",
      "0.08497695624828339\n",
      "0.14626924693584442\n",
      "0.06205281242728233\n",
      "0.12067064642906189\n",
      "0.1185108944773674\n",
      "0.08407816290855408\n",
      "0.1196126714348793\n",
      "0.10668584704399109\n",
      "0.11334957182407379\n",
      "0.08418522775173187\n",
      "0.16062909364700317\n",
      "0.08717826008796692\n",
      "0.1411266326904297\n",
      "0.1073378175497055\n",
      "0.0979529470205307\n",
      "0.1898629367351532\n",
      "0.08069606125354767\n",
      "0.09664791077375412\n",
      "0.06209591031074524\n",
      "0.24053137004375458\n",
      "0.10244683921337128\n",
      "0.11473314464092255\n",
      "0.12750142812728882\n",
      "0.09539580345153809\n",
      "0.148973286151886\n",
      "0.1438688337802887\n",
      "0.15634900331497192\n",
      "0.11989159882068634\n",
      "0.12491926550865173\n",
      "0.10823459923267365\n",
      "0.10856039077043533\n",
      "0.10831291973590851\n",
      "0.12580646574497223\n",
      "0.0936242863535881\n",
      "0.10131777077913284\n",
      "0.11293091624975204\n",
      "0.11820206791162491\n",
      "0.17767219245433807\n",
      "0.15050415694713593\n",
      "0.10618121176958084\n",
      "0.08619999140501022\n",
      "0.08180715888738632\n",
      "0.20810483396053314\n",
      "0.11457404494285583\n",
      "0.09286049753427505\n",
      "0.1591821312904358\n",
      "0.15824908018112183\n",
      "0.10136966407299042\n",
      "0.0952293798327446\n",
      "0.12728703022003174\n",
      "0.058802440762519836\n",
      "0.1034478172659874\n",
      "0.1058109849691391\n",
      "0.18152406811714172\n",
      "0.1368938386440277\n",
      "0.1391780972480774\n",
      "0.09403302520513535\n",
      "0.10191242396831512\n",
      "0.1599479615688324\n",
      "0.11410808563232422\n",
      "0.12424179166555405\n",
      "0.10463005304336548\n",
      "0.1397077739238739\n",
      "0.16716428101062775\n",
      "0.09095367789268494\n",
      "0.12280361354351044\n",
      "0.18957549333572388\n",
      "0.12592937052249908\n",
      "0.08316166698932648\n",
      "0.11573909968137741\n",
      "0.07929647713899612\n",
      "0.10819250345230103\n",
      "0.11211780458688736\n",
      "0.10152289271354675\n",
      "0.12041538208723068\n",
      "0.09981955587863922\n",
      "0.17352460324764252\n",
      "0.08453857153654099\n",
      "0.19001154601573944\n",
      "0.10413366556167603\n",
      "0.1538131684064865\n",
      "0.16198275983333588\n",
      "0.11284555494785309\n",
      "0.19716787338256836\n",
      "0.14263567328453064\n",
      "0.10194788128137589\n",
      "0.16077926754951477\n",
      "0.09853799641132355\n",
      "0.14158016443252563\n",
      "0.07286693900823593\n",
      "0.10513393580913544\n",
      "0.19139669835567474\n",
      "0.08333327621221542\n",
      "0.1631888896226883\n",
      "0.09228181838989258\n",
      "0.12870419025421143\n",
      "0.12644809484481812\n",
      "0.12327125668525696\n",
      "0.1066775694489479\n",
      "0.09598768502473831\n",
      "0.09637688845396042\n",
      "0.19019441306591034\n",
      "0.09606312960386276\n",
      "0.14434264600276947\n",
      "0.10911204665899277\n",
      "0.13949793577194214\n",
      "0.1729636937379837\n",
      "0.08446420729160309\n",
      "0.13407720625400543\n",
      "0.09087161719799042\n",
      "0.2373981922864914\n",
      "0.12772691249847412\n",
      "0.1229654997587204\n",
      "0.10837940126657486\n",
      "0.161532461643219\n",
      "0.1078530102968216\n",
      "0.08466500043869019\n",
      "0.11690385639667511\n",
      "0.0803714171051979\n",
      "0.11207042634487152\n",
      "0.12756747007369995\n",
      "0.11262498795986176\n",
      "0.10425221174955368\n",
      "0.111390620470047\n",
      "0.13159991800785065\n",
      "0.13150237500667572\n",
      "0.17160579562187195\n",
      "0.0861494317650795\n",
      "0.10587116330862045\n",
      "0.07455355674028397\n",
      "0.16893140971660614\n",
      "0.0947071760892868\n",
      "0.06402728706598282\n",
      "0.08041862398386002\n",
      "0.09165732562541962\n",
      "0.14026015996932983\n",
      "0.09658241271972656\n",
      "0.10213541239500046\n",
      "0.11211373656988144\n",
      "0.2343403398990631\n",
      "0.1205742284655571\n",
      "0.15097106993198395\n",
      "0.12380725890398026\n",
      "0.06712725013494492\n",
      "0.15472111105918884\n",
      "0.2364373654127121\n",
      "0.0922679677605629\n",
      "0.16627565026283264\n",
      "0.13103458285331726\n",
      "0.12584823369979858\n",
      "0.120191290974617\n",
      "0.1705740988254547\n",
      "0.13869792222976685\n",
      "0.07923508435487747\n",
      "0.1070416048169136\n",
      "0.17730222642421722\n",
      "0.10021349787712097\n",
      "0.14589455723762512\n",
      "0.1053437814116478\n",
      "0.11329002678394318\n",
      "0.08138904720544815\n",
      "0.10402620583772659\n",
      "0.14829745888710022\n",
      "0.10018827766180038\n",
      "0.09113403409719467\n",
      "0.11636035144329071\n",
      "0.17859363555908203\n",
      "0.14679056406021118\n",
      "0.16192755103111267\n",
      "0.1144552230834961\n",
      "0.0987408459186554\n",
      "0.10749725252389908\n",
      "0.10288643091917038\n",
      "0.13908427953720093\n",
      "0.08870386332273483\n",
      "0.1238417997956276\n",
      "0.08931663632392883\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5 + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
